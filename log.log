(nlp) ubuntu@gpu-1:/data/ydh/nlp$ python fixed_vllm.py 
loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--sentence-transformers--multi-qa-MiniLM-L6-cos-v1/snapshots/b207367332321f8e44f96e224ef15bc607f4dbf0/config.json
Model config BertConfig {
  "_name_or_path": "sentence-transformers/multi-qa-MiniLM-L6-cos-v1",
  "architectures": [
    "BertModel"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 384,
  "initializer_range": 0.02,
  "intermediate_size": 1536,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 6,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.46.3",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading weights file model.safetensors from cache at /home/ubuntu/.cache/huggingface/hub/models--sentence-transformers--multi-qa-MiniLM-L6-cos-v1/snapshots/b207367332321f8e44f96e224ef15bc607f4dbf0/model.safetensors
All model checkpoint weights were used when initializing BertModel.

All the weights of BertModel were initialized from the model checkpoint at sentence-transformers/multi-qa-MiniLM-L6-cos-v1.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
loading file vocab.txt from cache at /home/ubuntu/.cache/huggingface/hub/models--sentence-transformers--multi-qa-MiniLM-L6-cos-v1/snapshots/b207367332321f8e44f96e224ef15bc607f4dbf0/vocab.txt
loading file tokenizer.json from cache at /home/ubuntu/.cache/huggingface/hub/models--sentence-transformers--multi-qa-MiniLM-L6-cos-v1/snapshots/b207367332321f8e44f96e224ef15bc607f4dbf0/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/ubuntu/.cache/huggingface/hub/models--sentence-transformers--multi-qa-MiniLM-L6-cos-v1/snapshots/b207367332321f8e44f96e224ef15bc607f4dbf0/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--sentence-transformers--multi-qa-MiniLM-L6-cos-v1/snapshots/b207367332321f8e44f96e224ef15bc607f4dbf0/tokenizer_config.json
loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--google--pegasus-large/snapshots/dec7796b22f29b7d1c476192313eae8ed57b6b77/config.json
Model config PegasusConfig {
  "_name_or_path": "google/pegasus-large",
  "activation_dropout": 0.1,
  "activation_function": "relu",
  "add_bias_logits": false,
  "add_final_layer_norm": true,
  "architectures": [
    "PegasusForConditionalGeneration"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "classifier_dropout": 0.0,
  "d_model": 1024,
  "decoder_attention_heads": 16,
  "decoder_ffn_dim": 4096,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 16,
  "decoder_start_token_id": 0,
  "dropout": 0.1,
  "encoder_attention_heads": 16,
  "encoder_ffn_dim": 4096,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 16,
  "eos_token_id": 1,
  "extra_pos_embeddings": 1,
  "force_bos_token_to_be_generated": false,
  "forced_eos_token_id": 1,
  "gradient_checkpointing": false,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "length_penalty": 0.8,
  "max_length": 256,
  "max_position_embeddings": 1024,
  "model_type": "pegasus",
  "normalize_before": true,
  "normalize_embedding": false,
  "num_beams": 8,
  "num_hidden_layers": 16,
  "pad_token_id": 0,
  "scale_embedding": true,
  "static_position_embeddings": true,
  "task_specific_params": {
    "summarization_aeslc": {
      "length_penalty": 0.6,
      "max_length": 32,
      "max_position_embeddings": 512
    },
    "summarization_arxiv": {
      "length_penalty": 0.8,
      "max_length": 256,
      "max_position_embeddings": 1024
    },
    "summarization_big_patent": {
      "length_penalty": 0.7,
      "max_length": 256,
      "max_position_embeddings": 1024
    },
    "summarization_billsum": {
      "length_penalty": 0.6,
      "max_length": 256,
      "max_position_embeddings": 1024
    },
    "summarization_cnn_dailymail": {
      "length_penalty": 0.8,
      "max_length": 128,
      "max_position_embeddings": 1024
    },
    "summarization_gigaword": {
      "length_penalty": 0.6,
      "max_length": 32,
      "max_position_embeddings": 128
    },
    "summarization_large": {
      "length_penalty": 0.8,
      "max_length": 256,
      "max_position_embeddings": 1024
    },
    "summarization_multi_news": {
      "length_penalty": 0.8,
      "max_length": 256,
      "max_position_embeddings": 1024
    },
    "summarization_newsroom": {
      "length_penalty": 0.8,
      "max_length": 128,
      "max_position_embeddings": 512
    },
    "summarization_pubmed": {
      "length_penalty": 0.8,
      "max_length": 256,
      "max_position_embeddings": 1024
    },
    "summarization_reddit_tifu": {
      "length_penalty": 0.6,
      "max_length": 128,
      "max_position_embeddings": 512
    },
    "summarization_wikihow": {
      "length_penalty": 0.6,
      "max_length": 256,
      "max_position_embeddings": 512
    },
    "summarization_xsum": {
      "length_penalty": 0.8,
      "max_length": 64,
      "max_position_embeddings": 512
    }
  },
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 96103
}

loading weights file pytorch_model.bin from cache at /home/ubuntu/.cache/huggingface/hub/models--google--pegasus-large/snapshots/dec7796b22f29b7d1c476192313eae8ed57b6b77/pytorch_model.bin
Attempting to create safetensors variant
Attempting to convert .bin model on the fly to safetensors.
Generate config GenerationConfig {
  "bos_token_id": 0,
  "decoder_start_token_id": 0,
  "eos_token_id": 1,
  "forced_eos_token_id": 1,
  "length_penalty": 0.8,
  "max_length": 256,
  "num_beams": 8,
  "pad_token_id": 0
}

All model checkpoint weights were used when initializing PegasusForConditionalGeneration.

Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-large and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file generation_config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--google--pegasus-large/snapshots/dec7796b22f29b7d1c476192313eae8ed57b6b77/generation_config.json
Generate config GenerationConfig {
  "bos_token_id": 0,
  "decoder_start_token_id": 0,
  "eos_token_id": 1,
  "forced_eos_token_id": 1,
  "length_penalty": 0.8,
  "max_length": 256,
  "num_beams": 8,
  "pad_token_id": 0
}

loading file spiece.model from cache at /home/ubuntu/.cache/huggingface/hub/models--google--pegasus-large/snapshots/dec7796b22f29b7d1c476192313eae8ed57b6b77/spiece.model
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/ubuntu/.cache/huggingface/hub/models--google--pegasus-large/snapshots/dec7796b22f29b7d1c476192313eae8ed57b6b77/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--google--pegasus-large/snapshots/dec7796b22f29b7d1c476192313eae8ed57b6b77/tokenizer_config.json
loading file tokenizer.json from cache at None
loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--google--pegasus-large/snapshots/dec7796b22f29b7d1c476192313eae8ed57b6b77/config.json
Model config PegasusConfig {
  "_name_or_path": "google/pegasus-large",
  "activation_dropout": 0.1,
  "activation_function": "relu",
  "add_bias_logits": false,
  "add_final_layer_norm": true,
  "architectures": [
    "PegasusForConditionalGeneration"
  ],
  "attention_dropout": 0.1,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "classifier_dropout": 0.0,
  "d_model": 1024,
  "decoder_attention_heads": 16,
  "decoder_ffn_dim": 4096,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 16,
  "decoder_start_token_id": 0,
  "dropout": 0.1,
  "encoder_attention_heads": 16,
  "encoder_ffn_dim": 4096,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 16,
  "eos_token_id": 1,
  "extra_pos_embeddings": 1,
  "force_bos_token_to_be_generated": false,
  "forced_eos_token_id": 1,
  "gradient_checkpointing": false,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "length_penalty": 0.8,
  "max_length": 256,
  "max_position_embeddings": 1024,
  "model_type": "pegasus",
  "normalize_before": true,
  "normalize_embedding": false,
  "num_beams": 8,
  "num_hidden_layers": 16,
  "pad_token_id": 0,
  "scale_embedding": true,
  "static_position_embeddings": true,
  "task_specific_params": {
    "summarization_aeslc": {
      "length_penalty": 0.6,
      "max_length": 32,
      "max_position_embeddings": 512
    },
    "summarization_arxiv": {
      "length_penalty": 0.8,
      "max_length": 256,
      "max_position_embeddings": 1024
    },
    "summarization_big_patent": {
      "length_penalty": 0.7,
      "max_length": 256,
      "max_position_embeddings": 1024
    },
    "summarization_billsum": {
      "length_penalty": 0.6,
      "max_length": 256,
      "max_position_embeddings": 1024
    },
    "summarization_cnn_dailymail": {
      "length_penalty": 0.8,
      "max_length": 128,
      "max_position_embeddings": 1024
    },
    "summarization_gigaword": {
      "length_penalty": 0.6,
      "max_length": 32,
      "max_position_embeddings": 128
    },
    "summarization_large": {
      "length_penalty": 0.8,
      "max_length": 256,
      "max_position_embeddings": 1024
    },
    "summarization_multi_news": {
      "length_penalty": 0.8,
      "max_length": 256,
      "max_position_embeddings": 1024
    },
    "summarization_newsroom": {
      "length_penalty": 0.8,
      "max_length": 128,
      "max_position_embeddings": 512
    },
    "summarization_pubmed": {
      "length_penalty": 0.8,
      "max_length": 256,
      "max_position_embeddings": 1024
    },
    "summarization_reddit_tifu": {
      "length_penalty": 0.6,
      "max_length": 128,
      "max_position_embeddings": 512
    },
    "summarization_wikihow": {
      "length_penalty": 0.6,
      "max_length": 256,
      "max_position_embeddings": 512
    },
    "summarization_xsum": {
      "length_penalty": 0.8,
      "max_length": 64,
      "max_position_embeddings": 512
    }
  },
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 96103
}

loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8/config.json
loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8/config.json
Model config LlamaConfig {
  "_name_or_path": "deepseek-ai/deepseek-llm-7b-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 100000,
  "eos_token_id": 100001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 30,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.3",
  "use_cache": true,
  "vocab_size": 102400
}

Could not locate the image processor configuration file, will try to use the model config instead.
INFO 05-21 17:57:59 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='deepseek-ai/deepseek-llm-7b-chat', speculative_config=None, tokenizer='deepseek-ai/deepseek-llm-7b-chat', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=deepseek-ai/deepseek-llm-7b-chat, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)
loading file tokenizer.model from cache at None
loading file tokenizer.json from cache at /home/ubuntu/.cache/huggingface/hub/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8/tokenizer_config.json
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
loading configuration file generation_config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--deepseek-ai--deepseek-llm-7b-chat/snapshots/afbda8b347ec881666061fa67447046fc5164ec8/generation_config.json
Generate config GenerationConfig {
  "bos_token_id": 100000,
  "do_sample": true,
  "eos_token_id": 100001,
  "temperature": 0.7,
  "top_p": 0.95
}

INFO 05-21 17:58:01 model_runner.py:1056] Starting to load model deepseek-ai/deepseek-llm-7b-chat...